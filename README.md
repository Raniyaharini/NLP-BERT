# ğŸ¤– Classification of Reflective Sentences in Higher Education

This project is part of the PetraKIP initiative, aiming to combine technology development and education through interdisciplinary research in artificial intelligence (AI). Specifically, this project focuses on **classifying reflective sentences** according to the **Gibbs reflective cycle**, using **Natural Language Processing (NLP)** and **machine learning** techniques.

---

## ğŸ§  Project Motivation

While AI offers promising tools for improving student learning experiences and addressing teacher shortages through individualized study support, it also raises **technology-related concerns and skepticism**, particularly among future educators. Understanding and **automating reflective practices** can help develop more adaptive, personalized, and effective educational technologies.

---

## ğŸ¯ Project Goals

- Annotate reflective text data based on the **Gibbs reflective cycle** stages.
- Develop and test various **NLP-based classification models** to predict sentence reflectiveness.
- Balance trade-offs between model robustness and limited data availability.
- Experiment creatively to discover optimal prediction strategies.

---

## ğŸ” Problem Statement

**How can AI classify the stages of reflection in student-generated text, given the constraints of limited data and the complexity of reflective thinking?**

This is important for:
- Improving teacher education
- Enhancing personalized learning support
- Contributing to more effective use of AI in educational contexts

---

## ğŸ› ï¸ Implementation & Tools

We are exploring various modeling approaches:

- **Text embeddings**: BERT, FastText
- **Representation learning**: Contrastive Predictive Coding (CPC)
- **Annotation schema**: Cognitive stages from **Gibbs Reflective Cycle**
- **Model types**: Traditional ML and deep learning

### Milestones

1. Annotate dataset based on reflection stages  
2. Preprocess and embed text data  
3. Train baseline models (e.g., logistic regression, SVM)  
4. Implement advanced models (e.g., BERT fine-tuning)  
5. Evaluate and compare performance  
6. Document results and prepare final report  

---

## ğŸ“Š Evaluation

- Performance metrics: **Accuracy**, **F1 Score**, **Confusion Matrix**
- Comparison to **baseline models**
- Error analysis to identify model weaknesses

---

## ğŸ“Œ Future Work

- Expand annotated dataset through crowd-sourcing or semi-supervised methods
- Incorporate domain adaptation for reflective writing in varied contexts
- Improve sentence-level embedding strategies
- Experiment with prompt-tuned LLMs (e.g., T5, GPT) for few-shot classification

---

## ğŸ”— Resources & Links

- ğŸ“š Key Literature:
  - BERT embeddings  
  - FastText  
  - Automated Prediction of Reflectiveness  
  - Representation Learning with Contrastive Predictive Coding  

---

## ğŸ“¬ Contact

**Raniyaharini Rajendran**  
ğŸ“§ r.raniyaharini@gmail.com

**Hyosang Kim**  
ğŸ“§ gytkd0303@gmail.com


---

## ğŸ”‘ Keywords

`AI-assisted reflection` â€¢ `Gibbs cycle` â€¢ `Reflective writing` â€¢ `NLP` â€¢ `BERT` â€¢ `Contrastive Learning`
